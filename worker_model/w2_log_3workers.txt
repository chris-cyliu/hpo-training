C:\Users\b148848\AppData\Local\Microsoft\WindowsApps\ubuntu.exe run "export PYTHONUNBUFFERED=1 && export PYTHONIOENCODING=UTF-8 && export \"PYTHONPATH=/mnt/d/GitRepository/GRPC-Model:/mnt/d/Program Files/JetBrains/PyCharm 2019.2.2/helpers/pycharm_matplotlib_backend:/mnt/d/Program Files/JetBrains/PyCharm 2019.2.2/helpers/pycharm_display\" && export PYCHARM_HOSTED=1 && export PYCHARM_DISPLAY_PORT=63342 && cd /mnt/d/GitRepository/GRPC-Model/worker_model && /home/vincent/miniconda3/envs/build/bin/python /mnt/d/GitRepository/GRPC-Model/worker_model/w2.py"
2019-11-12 16:47:37.220939: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2904000000 Hz
2019-11-12 16:47:37.221908: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fffcd567a80 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2019-11-12 16:47:37.222040: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
E1112 16:47:37.226547300   24551 socket_utils_common_posix.cc:198] check for SO_REUSEPORT: {"created":"@1573548457.226533800","description":"Protocol not available","errno":92,"file":"external/grpc/src/core/lib/iomgr/socket_utils_common_posix.cc","file_line":175,"os_error":"Protocol not available","syscall":"getsockopt(SO_REUSEPORT)"}
E1112 16:47:37.226686700   24551 socket_utils_common_posix.cc:299] setsockopt(TCP_USER_TIMEOUT) Protocol not available
2019-11-12 16:47:37.226904: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:300] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12345, 1 -> localhost:23456, 2 -> localhost:12346}
2019-11-12 16:47:37.230120: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:371] Started server with target: grpc://localhost:12346
INFO:tensorflow:Enabled multi-worker collective ops with available devices: ['/job:worker/replica:0/task:2/device:CPU:0', '/job:worker/replica:0/task:2/device:XLA_CPU:0']
INFO:tensorflow:Using MirroredStrategy with devices ('/job:worker/task:2',)
INFO:tensorflow:MultiWorkerMirroredStrategy with cluster_spec = {'worker': ['localhost:12345', 'localhost:23456', 'localhost:12346']}, task_type = 'worker', task_id = 2, num_workers = 3, local_devices = ('/job:worker/task:2',), communication = CollectiveCommunication.AUTO
INFO:tensorflow:TF_CONFIG environment variable: {'cluster': {'worker': ['localhost:12345', 'localhost:23456', 'localhost:12346']}, 'task': {'type': 'worker', 'index': 2}}
INFO:tensorflow:Initializing RunConfig with distribution strategies.
INFO:tensorflow:RunConfig initialized for Distribute Coordinator with INDEPENDENT_WORKER mode
INFO:tensorflow:Using config: {'_model_dir': './estimator/multiworker', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': <tensorflow.python.distribute.collective_all_reduce_strategy.CollectiveAllReduceStrategy object at 0x7f59a41c8950>, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({'worker': ['localhost:12345', 'localhost:23456', 'localhost:12346']}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_distribute_coordinator_mode': 'independent_worker'}
INFO:tensorflow:Running `train_and_evaluate` with Distribute Coordinator.
start training and evaluating
INFO:tensorflow:Running Distribute Coordinator with mode = 'independent_worker', cluster_spec = {'worker': ['localhost:12345', 'localhost:23456', 'localhost:12346']}, task_type = 'worker', task_id = 2, environment = None, rpc_layer = 'grpc'
WARNING:tensorflow:`eval_strategy` is not passed in. No distribution strategy will be used for evaluation.
INFO:tensorflow:Using MirroredStrategy with devices ('/job:worker/task:2',)
INFO:tensorflow:MultiWorkerMirroredStrategy with cluster_spec = {'worker': ['localhost:12345', 'localhost:23456', 'localhost:12346']}, task_type = 'worker', task_id = 2, num_workers = 3, local_devices = ('/job:worker/task:2',), communication = CollectiveCommunication.AUTO
INFO:tensorflow:Using MirroredStrategy with devices ('/job:worker/task:2',)
INFO:tensorflow:MultiWorkerMirroredStrategy with cluster_spec = {'worker': ['localhost:12345', 'localhost:23456', 'localhost:12346']}, task_type = 'worker', task_id = 2, num_workers = 3, local_devices = ('/job:worker/task:2',), communication = CollectiveCommunication.AUTO
INFO:tensorflow:Updated config: {'_model_dir': './estimator/multiworker', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': <tensorflow.python.distribute.collective_all_reduce_strategy.CollectiveAllReduceStrategy object at 0x7f59a41c8350>, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({'worker': ['localhost:12345', 'localhost:23456', 'localhost:12346']}), '_task_type': 'worker', '_task_id': 2, '_global_id_in_cluster': 2, '_master': 'grpc://localhost:12346', '_evaluation_master': 'grpc://localhost:12346', '_is_chief': False, '_num_ps_replicas': 0, '_num_worker_replicas': 3, '_distribute_coordinator_mode': 'independent_worker'}
INFO:tensorflow:The `input_fn` accepts an `input_context` which will be given by DistributionStrategy
WARNING:tensorflow:From /home/vincent/miniconda3/envs/build/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1634: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
WARNING:tensorflow:From /home/vincent/miniconda3/envs/build/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1634: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Collective batch_all_reduce: 6 all-reduces, num_workers = 3, communication_hint = AUTO
INFO:tensorflow:Collective batch_all_reduce: 6 all-reduces, num_workers = 3, communication_hint = AUTO
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_workers = 3, communication_hint = AUTO
INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_workers = 3, communication_hint = AUTO
WARNING:tensorflow:Collective ops may deadlock with `save_checkpoints_secs` please use `save_checkpoint_steps` instead. Clearing `save_checkpoint_secs` and setting `save_checkpoint_steps` to 1000 now.
WARNING:tensorflow:Collective ops may deadlock with `save_checkpoints_secs` please use `save_checkpoint_steps` instead. Clearing `save_checkpoint_secs` and setting `save_checkpoint_steps` to 1000 now.
INFO:tensorflow:Create CheckpointSaverHook.
INFO:tensorflow:Create CheckpointSaverHook.
INFO:tensorflow:all_hooks [<tensorflow_estimator.python.estimator.util.DistributedIteratorInitializerHook object at 0x7f59a41de8d0>, <tensorflow.python.training.basic_session_run_hooks.StopAtStepHook object at 0x7f59a41d5450>, <tensorflow.python.training.basic_session_run_hooks.NanTensorHook object at 0x7f599c6906d0>, <tensorflow.python.training.basic_session_run_hooks.LoggingTensorHook object at 0x7f599c669490>, <tensorflow.python.training.basic_session_run_hooks.StepCounterHook object at 0x7f599c75bf10>, <tensorflow.python.training.basic_session_run_hooks.SummarySaverHook object at 0x7f599c5da110>, <tensorflow.python.training.basic_session_run_hooks.CheckpointSaverHook object at 0x7f59a404c750>]
INFO:tensorflow:all_hooks [<tensorflow_estimator.python.estimator.util.DistributedIteratorInitializerHook object at 0x7f59a41de8d0>, <tensorflow.python.training.basic_session_run_hooks.StopAtStepHook object at 0x7f59a41d5450>, <tensorflow.python.training.basic_session_run_hooks.NanTensorHook object at 0x7f599c6906d0>, <tensorflow.python.training.basic_session_run_hooks.LoggingTensorHook object at 0x7f599c669490>, <tensorflow.python.training.basic_session_run_hooks.StepCounterHook object at 0x7f599c75bf10>, <tensorflow.python.training.basic_session_run_hooks.SummarySaverHook object at 0x7f599c5da110>, <tensorflow.python.training.basic_session_run_hooks.CheckpointSaverHook object at 0x7f59a404c750>]
INFO:tensorflow:Creating chief session creator with config: device_filters: "/job:worker/task:2"
allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
    scoped_allocator_optimization: ON
    scoped_allocator_opts {
      enable_op: "CollectiveReduce"
    }
  }
}
experimental {
  collective_group_leader: "/job:worker/replica:0/task:0"
}

INFO:tensorflow:Creating chief session creator with config: device_filters: "/job:worker/task:2"
allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
    scoped_allocator_optimization: ON
    scoped_allocator_opts {
      enable_op: "CollectiveReduce"
    }
  }
}
experimental {
  collective_group_leader: "/job:worker/replica:0/task:0"
}

INFO:tensorflow:Graph was finalized.
INFO:tensorflow:Graph was finalized.
2019-11-12 16:47:37.827330: W tensorflow/core/common_runtime/collective_param_resolver_local.cc:182] Collective op conv2d/bias/Initializer/CollectiveBcastRecv: Broadcast(0) got duplicate CompleteGroup calls for group 1 and device /job:worker/replica:0/task:2/device:CPU:0
2019-11-12 16:47:37.827501: W tensorflow/core/common_runtime/collective_param_resolver_local.cc:182] Collective op conv2d/kernel/Initializer/CollectiveBcastRecv: Broadcast(0) got duplicate CompleteGroup calls for group 1 and device /job:worker/replica:0/task:2/device:CPU:0
2019-11-12 16:47:37.827642: W tensorflow/core/common_runtime/collective_param_resolver_local.cc:182] Collective op dense/bias/Initializer/CollectiveBcastRecv: Broadcast(0) got duplicate CompleteGroup calls for group 1 and device /job:worker/replica:0/task:2/device:CPU:0
2019-11-12 16:47:37.827803: W tensorflow/core/common_runtime/collective_param_resolver_local.cc:182] Collective op global_step/Initializer/CollectiveBcastRecv: Broadcast(0) got duplicate CompleteGroup calls for group 1 and device /job:worker/replica:0/task:2/device:CPU:0
2019-11-12 16:47:37.827957: W tensorflow/core/common_runtime/collective_param_resolver_local.cc:182] Collective op dense_1/bias/Initializer/CollectiveBcastRecv: Broadcast(0) got duplicate CompleteGroup calls for group 1 and device /job:worker/replica:0/task:2/device:CPU:0
2019-11-12 16:47:37.828127: W tensorflow/core/common_runtime/collective_param_resolver_local.cc:182] Collective op dense_1/kernel/Initializer/CollectiveBcastRecv: Broadcast(0) got duplicate CompleteGroup calls for group 1 and device /job:worker/replica:0/task:2/device:CPU:0
2019-11-12 16:47:37.828361: W tensorflow/core/common_runtime/collective_param_resolver_local.cc:182] Collective op dense/kernel/Initializer/CollectiveBcastRecv: Broadcast(0) got duplicate CompleteGroup calls for group 1 and device /job:worker/replica:0/task:2/device:CPU:0
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Saving checkpoints for 0 into ./estimator/multiworker/tmp_worker_2/model.ckpt.
INFO:tensorflow:Saving checkpoints for 0 into ./estimator/multiworker/tmp_worker_2/model.ckpt.
2019-11-12 16:47:42.892601: W tensorflow/core/common_runtime/collective_param_resolver_local.cc:182] Collective op allreduce_1/CollectiveReduce: Reduce(Add,Id) got duplicate CompleteGroup calls for group 1 and device /job:worker/replica:0/task:2/device:CPU:0
2019-11-12 16:47:42.892925: W tensorflow/core/common_runtime/collective_param_resolver_local.cc:182] Collective op allreduce/CollectiveReduce_5: Reduce(Add,Id) got duplicate CompleteGroup calls for group 1 and device /job:worker/replica:0/task:2/device:CPU:0
2019-11-12 16:47:42.893116: W tensorflow/core/common_runtime/collective_param_resolver_local.cc:182] Collective op allreduce/CollectiveReduce_4: Reduce(Add,Id) got duplicate CompleteGroup calls for group 1 and device /job:worker/replica:0/task:2/device:CPU:0
2019-11-12 16:47:42.893256: W tensorflow/core/common_runtime/collective_param_resolver_local.cc:182] Collective op allreduce/CollectiveReduce_3: Reduce(Add,Id) got duplicate CompleteGroup calls for group 1 and device /job:worker/replica:0/task:2/device:CPU:0
2019-11-12 16:47:42.898972: W tensorflow/core/common_runtime/collective_param_resolver_local.cc:182] Collective op allreduce/CollectiveReduce_2: Reduce(Add,Id) got duplicate CompleteGroup calls for group 1 and device /job:worker/replica:0/task:2/device:CPU:0
2019-11-12 16:47:42.905668: W tensorflow/core/common_runtime/collective_param_resolver_local.cc:182] Collective op allreduce/CollectiveReduce_1: Reduce(Add,Id) got duplicate CompleteGroup calls for group 1 and device /job:worker/replica:0/task:2/device:CPU:0
2019-11-12 16:47:42.909516: W tensorflow/core/common_runtime/collective_param_resolver_local.cc:182] Collective op allreduce/CollectiveReduce: Reduce(Add,Id) got duplicate CompleteGroup calls for group 1 and device /job:worker/replica:0/task:2/device:CPU:0
INFO:tensorflow:loss = 2.3260744, step = 0
INFO:tensorflow:loss = 2.3260744, step = 0
INFO:tensorflow:loss = 2.3133733, step = 100 (5.211 sec)
INFO:tensorflow:loss = 2.3133733, step = 100 (5.211 sec)
INFO:tensorflow:global_step/sec: 19.1873
INFO:tensorflow:global_step/sec: 19.1873
2019-11-12 16:47:51.217359: E tensorflow/core/common_runtime/ring_alg.cc:279] Aborting RingReduce with Unavailable: Socket closed
Additional GRPC error information:
{"created":"@1573548471.217064500","description":"Error received from peer","file":"external/grpc/src/core/lib/surface/call.cc","file_line":1039,"grpc_message":"Socket closed","grpc_status":14}
2019-11-12 16:47:51.217614: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Unavailable: Socket closed
Additional GRPC error information:
{"created":"@1573548471.217064500","description":"Error received from peer","file":"external/grpc/src/core/lib/surface/call.cc","file_line":1039,"grpc_message":"Socket closed","grpc_status":14}
2019-11-12 16:47:51.218033: E tensorflow/core/common_runtime/ring_alg.cc:279] Aborting RingReduce with Unavailable: Socket closed
Additional GRPC error information:
{"created":"@1573548471.217091600","description":"Error received from peer","file":"external/grpc/src/core/lib/surface/call.cc","file_line":1039,"grpc_message":"Socket closed","grpc_status":14}
2019-11-12 16:47:51.218166: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Unavailable: Socket closed
Additional GRPC error information:
{"created":"@1573548471.217091600","description":"Error received from peer","file":"external/grpc/src/core/lib/surface/call.cc","file_line":1039,"grpc_message":"Socket closed","grpc_status":14}
2019-11-12 16:47:51.218433: E tensorflow/core/common_runtime/ring_alg.cc:279] Aborting RingReduce with Unavailable: Socket closed
Additional GRPC error information:
{"created":"@1573548471.217098300","description":"Error received from peer","file":"external/grpc/src/core/lib/surface/call.cc","file_line":1039,"grpc_message":"Socket closed","grpc_status":14}
2019-11-12 16:47:51.218556: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Unavailable: Socket closed
Additional GRPC error information:
{"created":"@1573548471.217098300","description":"Error received from peer","file":"external/grpc/src/core/lib/surface/call.cc","file_line":1039,"grpc_message":"Socket closed","grpc_status":14}
2019-11-12 16:47:51.218700: E tensorflow/core/common_runtime/ring_alg.cc:279] Aborting RingReduce with Unavailable: Socket closed
Additional GRPC error information:
{"created":"@1573548471.217110300","description":"Error received from peer","file":"external/grpc/src/core/lib/surface/call.cc","file_line":1039,"grpc_message":"Socket closed","grpc_status":14}
2019-11-12 16:47:51.218758: W tensorflow/core/framework/op_kernel.cc:1655] OP_REQUIRES failed at collective_ops.cc:253 : Unavailable: Socket closed
Additional GRPC error information:
{"created":"@1573548471.217091600","description":"Error received from peer","file":"external/grpc/src/core/lib/surface/call.cc","file_line":1039,"grpc_message":"Socket closed","grpc_status":14}
2019-11-12 16:47:51.218963: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Unavailable: Socket closed
Additional GRPC error information:
{"created":"@1573548471.217091600","description":"Error received from peer","file":"external/grpc/src/core/lib/surface/call.cc","file_line":1039,"grpc_message":"Socket closed","grpc_status":14}
	 [[{{node allreduce/CollectiveReduce_4}}]]
2019-11-12 16:47:51.218819: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Unavailable: Socket closed
Additional GRPC error information:
{"created":"@1573548471.217110300","description":"Error received from peer","file":"external/grpc/src/core/lib/surface/call.cc","file_line":1039,"grpc_message":"Socket closed","grpc_status":14}
2019-11-12 16:47:51.219775: W tensorflow/core/framework/op_kernel.cc:1655] OP_REQUIRES failed at collective_ops.cc:253 : Unavailable: Socket closed
Additional GRPC error information:
{"created":"@1573548471.217064500","description":"Error received from peer","file":"external/grpc/src/core/lib/surface/call.cc","file_line":1039,"grpc_message":"Socket closed","grpc_status":14}
2019-11-12 16:47:51.219849: W tensorflow/core/framework/op_kernel.cc:1655] OP_REQUIRES failed at collective_ops.cc:253 : Unavailable: Socket closed
Additional GRPC error information:
{"created":"@1573548471.217110300","description":"Error received from peer","file":"external/grpc/src/core/lib/surface/call.cc","file_line":1039,"grpc_message":"Socket closed","grpc_status":14}
2019-11-12 16:47:51.220008: W tensorflow/core/framework/op_kernel.cc:1655] OP_REQUIRES failed at collective_ops.cc:253 : Unavailable: Socket closed
Additional GRPC error information:
{"created":"@1573548471.217098300","description":"Error received from peer","file":"external/grpc/src/core/lib/surface/call.cc","file_line":1039,"grpc_message":"Socket closed","grpc_status":14}
INFO:tensorflow:An error was raised. This may be due to a preemption in a connected worker or parameter server. The current session will be closed and a new session will be created. This error may also occur due to a gRPC failure caused by high memory or network bandwidth usage in the parameter servers. If this error occurs repeatedly, try increasing the number of parameter servers assigned to the job. Error: From /job:worker/replica:0/task:2:
Socket closed
Additional GRPC error information:
{"created":"@1573548471.217091600","description":"Error received from peer","file":"external/grpc/src/core/lib/surface/call.cc","file_line":1039,"grpc_message":"Socket closed","grpc_status":14}
	 [[node allreduce/CollectiveReduce_4 (defined at home/vincent/miniconda3/envs/build/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py:1301) ]]

Errors may have originated from an input operation.
Input Source operations connected to node allreduce/CollectiveReduce_4:
 gradients/sequential/dense_1/MatMul_grad/tuple/control_dependency_1 (defined at mnt/d/GitRepository/GRPC-Model/worker_model/w2.py:69)

Original stack trace for 'allreduce/CollectiveReduce_4':
  File "mnt/d/GitRepository/GRPC-Model/worker_model/w2.py", line 86, in <module>
    eval_spec=tf.estimator.EvalSpec(input_fn=input_fn)
  File "home/vincent/miniconda3/envs/build/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/training.py", line 464, in train_and_evaluate
    estimator, train_spec, eval_spec, _TrainingExecutor)
  File "home/vincent/miniconda3/envs/build/lib/python3.7/site-packages/tensorflow_core/python/distribute/estimator_training.py", line 290, in train_and_evaluate
    session_config=run_config.session_config)
  File "home/vincent/miniconda3/envs/build/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_coordinator.py", line 853, in run_distribute_coordinator
    task_id, session_config, rpc_layer)
  File "home/vincent/miniconda3/envs/build/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_coordinator.py", line 360, in _run_single_worker
    return worker_fn(strategy)
  File "home/vincent/miniconda3/envs/build/lib/python3.7/site-packages/tensorflow_core/python/distribute/estimator_training.py", line 252, in _worker_fn
    hooks=hooks)
  File "home/vincent/miniconda3/envs/build/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py", line 370, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File "home/vincent/miniconda3/envs/build/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py", line 1158, in _train_model
    return self._train_model_distributed(input_fn, hooks, saving_listeners)
  File "home/vincent/miniconda3/envs/build/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py", line 1221, in _train_model_distributed
    self._config._train_distribute, input_fn, hooks, saving_listeners)
  File "home/vincent/miniconda3/envs/build/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py", line 1301, in _actual_train_model_distributed
    self.config))
  File "home/vincent/miniconda3/envs/build/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py", line 1819, in call_for_each_replica
    return self._call_for_each_replica(fn, args, kwargs)
  File "home/vincent/miniconda3/envs/build/lib/python3.7/site-packages/tensorflow_core/python/distribute/mirrored_strategy.py", line 688, in _call_for_each_replica
    fn, args, kwargs)
  File "home/vincent/miniconda3/envs/build/lib/python3.7/site-packages/tensorflow_core/python/distribute/mirrored_strategy.py", line 194, in _call_for_each_replica
    **merge_kwargs)
  File "home/vincent/miniconda3/envs/build/lib/python3.7/site-packages/tensorflow_core/python/training/optimizer.py", line 666, in _distributed_apply
    ds_reduce_util.ReduceOp.SUM, grads_and_vars)
  File "home/vincent/miniconda3/envs/build/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py", line 1494, in batch_reduce_to
    return self._batch_reduce_to(reduce_op, value_destination_pairs)
  File "home/vincent/miniconda3/envs/build/lib/python3.7/site-packages/tensorflow_core/python/distribute/mirrored_strategy.py", line 734, in _batch_reduce_to
    value_destination_pairs)
  File "home/vincent/miniconda3/envs/build/lib/python3.7/site-packages/tensorflow_core/python/distribute/cross_device_ops.py", line 324, in batch_reduce
    return self.batch_reduce_implementation(reduce_op, value_destination_pairs)
  File "home/vincent/miniconda3/envs/build/lib/python3.7/site-packages/tensorflow_core/python/distribute/cross_device_ops.py", line 1067, in batch_reduce_implementation
    [v[0] for v in value_destination_pairs])
  File "home/vincent/miniconda3/envs/build/lib/python3.7/site-packages/tensorflow_core/python/distribute/cross_device_ops.py", line 1115, in _batch_all_reduce
    dense_results = self._do_batch_all_reduce_dense(reduce_op, dense_values)
  File "home/vincent/miniconda3/envs/build/lib/python3.7/site-packages/tensorflow_core/python/distribute/cross_device_ops.py", line 1157, in _do_batch_all_reduce_dense
    "Id", communication_hint)
  File "home/vincent/miniconda3/envs/build/lib/python3.7/site-packages/tensorflow_core/python/distribute/cross_device_utils.py", line 368, in build_collective_reduce
    return collective_all_reduce()
  File "home/vincent/miniconda3/envs/build/lib/python3.7/site-packages/tensorflow_core/python/distribute/cross_device_utils.py", line 360, in collective_all_reduce
    unary_op, subdiv_offsets, communication_hint)
  File "home/vincent/miniconda3/envs/build/lib/python3.7/site-packages/tensorflow_core/python/ops/collective_ops.py", line 60, in all_reduce
    communication_hint=communication_hint.lower())
  File "home/vincent/miniconda3/envs/build/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_collective_ops.py", line 353, in collective_reduce
    communication_hint=communication_hint, name=name)
  File "home/vincent/miniconda3/envs/build/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py", line 742, in _apply_op_helper
    attrs=attr_protos, op_def=op_def)
  File "home/vincent/miniconda3/envs/build/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py", line 3305, in _create_op_internal
    op_def=op_def)
  File "home/vincent/miniconda3/envs/build/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py", line 1747, in __init__
    self._traceback = tf_stack.extract_stack()

INFO:tensorflow:An error was raised. This may be due to a preemption in a connected worker or parameter server. The current session will be closed and a new session will be created. This error may also occur due to a gRPC failure caused by high memory or network bandwidth usage in the parameter servers. If this error occurs repeatedly, try increasing the number of parameter servers assigned to the job. Error: From /job:worker/replica:0/task:2:
Socket closed
Additional GRPC error information:
{"created":"@1573548471.217091600","description":"Error received from peer","file":"external/grpc/src/core/lib/surface/call.cc","file_line":1039,"grpc_message":"Socket closed","grpc_status":14}
	 [[node allreduce/CollectiveReduce_4 (defined at home/vincent/miniconda3/envs/build/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py:1301) ]]

Errors may have originated from an input operation.
Input Source operations connected to node allreduce/CollectiveReduce_4:
 gradients/sequential/dense_1/MatMul_grad/tuple/control_dependency_1 (defined at mnt/d/GitRepository/GRPC-Model/worker_model/w2.py:69)

Original stack trace for 'allreduce/CollectiveReduce_4':
  File "mnt/d/GitRepository/GRPC-Model/worker_model/w2.py", line 86, in <module>
    eval_spec=tf.estimator.EvalSpec(input_fn=input_fn)
  File "home/vincent/miniconda3/envs/build/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/training.py", line 464, in train_and_evaluate
    estimator, train_spec, eval_spec, _TrainingExecutor)
  File "home/vincent/miniconda3/envs/build/lib/python3.7/site-packages/tensorflow_core/python/distribute/estimator_training.py", line 290, in train_and_evaluate
    session_config=run_config.session_config)
  File "home/vincent/miniconda3/envs/build/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_coordinator.py", line 853, in run_distribute_coordinator
    task_id, session_config, rpc_layer)
  File "home/vincent/miniconda3/envs/build/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_coordinator.py", line 360, in _run_single_worker
    return worker_fn(strategy)
  File "home/vincent/miniconda3/envs/build/lib/python3.7/site-packages/tensorflow_core/python/distribute/estimator_training.py", line 252, in _worker_fn
    hooks=hooks)
  File "home/vincent/miniconda3/envs/build/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py", line 370, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File "home/vincent/miniconda3/envs/build/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py", line 1158, in _train_model
    return self._train_model_distributed(input_fn, hooks, saving_listeners)
  File "home/vincent/miniconda3/envs/build/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py", line 1221, in _train_model_distributed
    self._config._train_distribute, input_fn, hooks, saving_listeners)
  File "home/vincent/miniconda3/envs/build/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py", line 1301, in _actual_train_model_distributed
    self.config))
  File "home/vincent/miniconda3/envs/build/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py", line 1819, in call_for_each_replica
    return self._call_for_each_replica(fn, args, kwargs)
  File "home/vincent/miniconda3/envs/build/lib/python3.7/site-packages/tensorflow_core/python/distribute/mirrored_strategy.py", line 688, in _call_for_each_replica
    fn, args, kwargs)
  File "home/vincent/miniconda3/envs/build/lib/python3.7/site-packages/tensorflow_core/python/distribute/mirrored_strategy.py", line 194, in _call_for_each_replica
    **merge_kwargs)
  File "home/vincent/miniconda3/envs/build/lib/python3.7/site-packages/tensorflow_core/python/training/optimizer.py", line 666, in _distributed_apply
    ds_reduce_util.ReduceOp.SUM, grads_and_vars)
  File "home/vincent/miniconda3/envs/build/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py", line 1494, in batch_reduce_to
    return self._batch_reduce_to(reduce_op, value_destination_pairs)
  File "home/vincent/miniconda3/envs/build/lib/python3.7/site-packages/tensorflow_core/python/distribute/mirrored_strategy.py", line 734, in _batch_reduce_to
    value_destination_pairs)
  File "home/vincent/miniconda3/envs/build/lib/python3.7/site-packages/tensorflow_core/python/distribute/cross_device_ops.py", line 324, in batch_reduce
    return self.batch_reduce_implementation(reduce_op, value_destination_pairs)
  File "home/vincent/miniconda3/envs/build/lib/python3.7/site-packages/tensorflow_core/python/distribute/cross_device_ops.py", line 1067, in batch_reduce_implementation
    [v[0] for v in value_destination_pairs])
  File "home/vincent/miniconda3/envs/build/lib/python3.7/site-packages/tensorflow_core/python/distribute/cross_device_ops.py", line 1115, in _batch_all_reduce
    dense_results = self._do_batch_all_reduce_dense(reduce_op, dense_values)
  File "home/vincent/miniconda3/envs/build/lib/python3.7/site-packages/tensorflow_core/python/distribute/cross_device_ops.py", line 1157, in _do_batch_all_reduce_dense
    "Id", communication_hint)
  File "home/vincent/miniconda3/envs/build/lib/python3.7/site-packages/tensorflow_core/python/distribute/cross_device_utils.py", line 368, in build_collective_reduce
    return collective_all_reduce()
  File "home/vincent/miniconda3/envs/build/lib/python3.7/site-packages/tensorflow_core/python/distribute/cross_device_utils.py", line 360, in collective_all_reduce
    unary_op, subdiv_offsets, communication_hint)
  File "home/vincent/miniconda3/envs/build/lib/python3.7/site-packages/tensorflow_core/python/ops/collective_ops.py", line 60, in all_reduce
    communication_hint=communication_hint.lower())
  File "home/vincent/miniconda3/envs/build/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_collective_ops.py", line 353, in collective_reduce
    communication_hint=communication_hint, name=name)
  File "home/vincent/miniconda3/envs/build/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py", line 742, in _apply_op_helper
    attrs=attr_protos, op_def=op_def)
  File "home/vincent/miniconda3/envs/build/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py", line 3305, in _create_op_internal
    op_def=op_def)
  File "home/vincent/miniconda3/envs/build/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py", line 1747, in __init__
    self._traceback = tf_stack.extract_stack()

INFO:tensorflow:Graph was finalized.
INFO:tensorflow:Graph was finalized.
2019-11-12 16:48:01.305672: I tensorflow/core/distributed_runtime/master.cc:268] CreateSession still waiting for response from worker: /job:worker/replica:0/task:1
[Important] We catch an exception
Cannot assign a device for operation tmp_global_step/IsInitialized/VarIsInitializedOp: node tmp_global_step/IsInitialized/VarIsInitializedOp (defined at home/vincent/miniconda3/envs/build/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py:1356)  was explicitly assigned to /job:worker/task:1 but available devices are [ /job:worker/replica:0/task:2/device:CPU:0, /job:worker/replica:0/task:2/device:XLA_CPU:0 ]. Make sure the device specification refers to a valid device.
	 [[tmp_global_step/IsInitialized/VarIsInitializedOp]]
2019-11-12 16:48:02.641936: W tensorflow/core/common_runtime/eager/context.cc:319] Unable to destroy server_ object, so releasing instead. Servers don't support clean shutdown.

Process finished with exit code 1
